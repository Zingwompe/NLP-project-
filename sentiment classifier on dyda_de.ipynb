{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d411cd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gguaquiere\\anaconda3\\envs\\NLP\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import datasets\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b15c6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bert\n",
    "from bert import BertModelLayer\n",
    "from bert.loader import StockBertConfig, map_stock_config_to_params, load_stock_weights\n",
    "from bert.tokenization.bert_tokenization import FullTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12b5989c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1118d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 42\n",
    "np.random.seed(random_seed)\n",
    "tf.random.set_seed(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b886cced",
   "metadata": {},
   "source": [
    "# **Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e83417f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train_test_val_data(name_dataset : str):\n",
    "\n",
    "    data = load_dataset('silicone',name_dataset)\n",
    "    train = pd.DataFrame(data['train'])\n",
    "    test = pd.DataFrame(data['test'])\n",
    "    val = pd.DataFrame(data['validation'])\n",
    "\n",
    "    return train, test, val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0803891f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset silicone (C:\\Users\\gguaquiere\\.cache\\huggingface\\datasets\\silicone\\dyda_e\\1.0.0\\af617406c94e3f78da85f7ea74ebfbd3f297a9665cb54adbae305b03bc4442a5)\n",
      "100%|██████████| 3/3 [00:00<00:00, 187.49it/s]\n"
     ]
    }
   ],
   "source": [
    "dyda_da_train, dyda_da_test, dyda_da_val = load_train_test_val_data(\"dyda_e\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a941193",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Utterance</th>\n",
       "      <th>Emotion</th>\n",
       "      <th>Dialogue_ID</th>\n",
       "      <th>Label</th>\n",
       "      <th>Idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>say , jim , how about going for a few beers af...</td>\n",
       "      <td>no emotion</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>you know that is tempting but is really not go...</td>\n",
       "      <td>no emotion</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>what do you mean ? it will help us to relax .</td>\n",
       "      <td>no emotion</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>do you really think so ? i don't . it will jus...</td>\n",
       "      <td>no emotion</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i guess you are right.but what shall we do ? i...</td>\n",
       "      <td>no emotion</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87165</th>\n",
       "      <td>i want a pair of locus .</td>\n",
       "      <td>no emotion</td>\n",
       "      <td>11117</td>\n",
       "      <td>4</td>\n",
       "      <td>87165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87166</th>\n",
       "      <td>take a look at the ones on display , please .</td>\n",
       "      <td>no emotion</td>\n",
       "      <td>11117</td>\n",
       "      <td>4</td>\n",
       "      <td>87166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87167</th>\n",
       "      <td>i need size 41 .</td>\n",
       "      <td>no emotion</td>\n",
       "      <td>11117</td>\n",
       "      <td>4</td>\n",
       "      <td>87167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87168</th>\n",
       "      <td>could i have the check , please ?</td>\n",
       "      <td>no emotion</td>\n",
       "      <td>11118</td>\n",
       "      <td>4</td>\n",
       "      <td>87168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87169</th>\n",
       "      <td>okay . i'll just be a minute .</td>\n",
       "      <td>no emotion</td>\n",
       "      <td>11118</td>\n",
       "      <td>4</td>\n",
       "      <td>87169</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>87170 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Utterance     Emotion  \\\n",
       "0      say , jim , how about going for a few beers af...  no emotion   \n",
       "1      you know that is tempting but is really not go...  no emotion   \n",
       "2          what do you mean ? it will help us to relax .  no emotion   \n",
       "3      do you really think so ? i don't . it will jus...  no emotion   \n",
       "4      i guess you are right.but what shall we do ? i...  no emotion   \n",
       "...                                                  ...         ...   \n",
       "87165                           i want a pair of locus .  no emotion   \n",
       "87166      take a look at the ones on display , please .  no emotion   \n",
       "87167                                   i need size 41 .  no emotion   \n",
       "87168                  could i have the check , please ?  no emotion   \n",
       "87169                     okay . i'll just be a minute .  no emotion   \n",
       "\n",
       "      Dialogue_ID  Label    Idx  \n",
       "0               1      4      0  \n",
       "1               1      4      1  \n",
       "2               1      4      2  \n",
       "3               1      4      3  \n",
       "4               1      4      4  \n",
       "...           ...    ...    ...  \n",
       "87165       11117      4  87165  \n",
       "87166       11117      4  87166  \n",
       "87167       11117      4  87167  \n",
       "87168       11118      4  87168  \n",
       "87169       11118      4  87169  \n",
       "\n",
       "[87170 rows x 5 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dyda_da_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "43249ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#on réduit la taille du dataset pour avoir un temps d'entraînement raisonnable\n",
    "\n",
    "train = dyda_da_train[['Utterance','Emotion']].sample(frac=0.05)\n",
    "test = dyda_da_test[['Utterance','Emotion']].sample(frac=0.05)\n",
    "val = dyda_da_val[['Utterance','Emotion']].sample(frac=0.06)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f97cea5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "no emotion    3556\n",
       "happiness      613\n",
       "surprise        68\n",
       "sadness         52\n",
       "anger           48\n",
       "disgust         15\n",
       "fear             6\n",
       "Name: Emotion, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.Emotion.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a0565c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"model\", exist_ok=True)\n",
    "bert_model_name=\"uncased_L-12_H-768_A-12\"\n",
    "bert_ckpt_dir = 'model/uncased_L-12_H-768_A-12'\n",
    "bert_ckpt_file = 'model/uncased_L-12_H-768_A-12/bert_model.ckpt'\n",
    "bert_config_file = 'model/uncased_L-12_H-768_A-12/bert_config.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52048236",
   "metadata": {},
   "source": [
    "# **Input Text Preparation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d9c41d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPreparation:\n",
    "    \n",
    "    text_column = \"Utterance\"\n",
    "    label_column = \"Emotion\"\n",
    "\n",
    "    def __init__(self, train, test, tokenizer: FullTokenizer, classes, max_seq_len=192):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_seq_len = 0\n",
    "        self.classes = classes\n",
    "\n",
    "        ((self.train_x, self.train_y), (self.test_x, self.test_y)) = map(self.prepare_data, [train, test])\n",
    "\n",
    "        print(\"max seq_len\", self.max_seq_len)\n",
    "        self.max_seq_len = min(self.max_seq_len, max_seq_len)\n",
    "        self.train_x, self.test_x = map(self.data_padding, [self.train_x, self.test_x])\n",
    "\n",
    "    def prepare_data(self, df):\n",
    "        x, y = [], []\n",
    "\n",
    "        for _, row in tqdm(df.iterrows()):\n",
    "            text, label = row[DataPreparation.text_column], row[DataPreparation.label_column]\n",
    "            tokens = self.tokenizer.tokenize(text)\n",
    "            tokens = [\"[CLS]\"] + tokens + [\"[SEP]\"]\n",
    "            token_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n",
    "            self.max_seq_len = max(self.max_seq_len, len(token_ids))\n",
    "            x.append(token_ids)\n",
    "            y.append(self.classes.index(label))\n",
    "\n",
    "        return np.array(x), np.array(y)\n",
    "\n",
    "    def data_padding(self, ids):\n",
    "        x = []\n",
    "        for input_ids in ids:\n",
    "            input_ids = input_ids[:min(len(input_ids), self.max_seq_len - 2)]\n",
    "            input_ids = input_ids + [0] * (self.max_seq_len - len(input_ids))\n",
    "            x.append(np.array(input_ids))\n",
    "        return np.array(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "99313c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = FullTokenizer(vocab_file=bert_ckpt_dir+\"/vocab.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632a663c",
   "metadata": {},
   "source": [
    "# **Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "503d1243",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_defination(max_seq_len, bert_ckpt_file):\n",
    "    \n",
    "    with tf.io.gfile.GFile(bert_config_file, \"r\") as reader:\n",
    "        bc = StockBertConfig.from_json_string(reader.read())\n",
    "        bert_params = map_stock_config_to_params(bc)\n",
    "        bert_params.adapter_size = None\n",
    "        bert = BertModelLayer.from_params(bert_params, name=\"bert\")\n",
    "        \n",
    "    input_ids = keras.layers.Input(shape=(max_seq_len, ), dtype='int32', name=\"input_ids\")\n",
    "    bert_output = bert(input_ids)\n",
    "\n",
    "    print(\"bert shape\", bert_output.shape)\n",
    "\n",
    "    cls_out = keras.layers.Lambda(lambda seq: seq[:, 0, :])(bert_output)\n",
    "    cls_out = keras.layers.Dropout(0.5)(cls_out)\n",
    "    logits = keras.layers.Dense(units=768, activation=\"tanh\")(cls_out)\n",
    "    logits = keras.layers.Dropout(0.5)(logits)\n",
    "    logits = keras.layers.Dense(units=len(classes), activation=\"softmax\")(logits)\n",
    "\n",
    "    model = keras.Model(inputs=input_ids, outputs=logits)\n",
    "    model.build(input_shape=(None, max_seq_len))\n",
    "\n",
    "    load_stock_weights(bert, bert_ckpt_file)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4571fd17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4358it [00:01, 3898.02it/s]\n",
      "C:\\Users\\gguaquiere\\anaconda3\\envs\\NLP\\lib\\site-packages\\ipykernel_launcher.py:29: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "387it [00:00, 4042.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max seq_len 156\n"
     ]
    }
   ],
   "source": [
    "classes = train.Emotion.unique().tolist()\n",
    "\n",
    "data = DataPreparation(train, test, tokenizer, classes, max_seq_len=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df107e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert shape (None, 128, 768)\n",
      "Done loading 196 BERT weights from: model/uncased_L-12_H-768_A-12/bert_model.ckpt into <bert.model.BertModelLayer object at 0x000001A933252860> (prefix:bert). Count of weights not found in the checkpoint was: [0]. Count of weights with mismatched shape: [0]\n",
      "Unused weights from checkpoint: \n",
      "\tbert/embeddings/token_type_embeddings\n",
      "\tbert/pooler/dense/bias\n",
      "\tbert/pooler/dense/kernel\n",
      "\tcls/predictions/output_bias\n",
      "\tcls/predictions/transform/LayerNorm/beta\n",
      "\tcls/predictions/transform/LayerNorm/gamma\n",
      "\tcls/predictions/transform/dense/bias\n",
      "\tcls/predictions/transform/dense/kernel\n",
      "\tcls/seq_relationship/output_bias\n",
      "\tcls/seq_relationship/output_weights\n"
     ]
    }
   ],
   "source": [
    "model = model_defination(data.max_seq_len, bert_ckpt_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "28a2833f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_ids (InputLayer)       [(None, 128)]             0         \n",
      "_________________________________________________________________\n",
      "bert (BertModelLayer)        (None, 128, 768)          108890112 \n",
      "_________________________________________________________________\n",
      "lambda (Lambda)              (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 768)               590592    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 7)                 5383      \n",
      "=================================================================\n",
      "Total params: 109,486,087\n",
      "Trainable params: 109,486,087\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b5802753",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "  optimizer=keras.optimizers.Adam(1e-5),\n",
    "  loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "  metrics=[keras.metrics.SparseCategoricalAccuracy(name=\"acc\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46716f09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3922 samples, validate on 436 samples\n",
      "Epoch 1/3\n",
      "3922/3922 [==============================] - 2406s 613ms/sample - loss: 1.3608 - acc: 0.8078 - val_loss: 1.3513 - val_acc: 0.8142\n",
      "Epoch 2/3\n",
      "3696/3922 [===========================>..] - ETA: 2:24 - loss: 1.3519 - acc: 0.8136"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "  x=data.train_x, \n",
    "  y=data.train_y,\n",
    "  validation_split=0.1,\n",
    "  batch_size=16,\n",
    "  shuffle=True,\n",
    "  epochs=3,\n",
    "  #callbacks=[tensorboard_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebcebaa7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b14e1d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668874e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60ff0c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654dbc14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad2f2e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05e1866",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df83afc2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbcb6a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da1de51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
